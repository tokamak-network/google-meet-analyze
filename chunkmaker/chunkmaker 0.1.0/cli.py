from __future__ import annotations

import argparse
import json
from pathlib import Path

from .chunking import chunk_text_from_normalized, normalize_text
from .io import discover_csv_files, iter_meeting_records
from .types import MeetingMeta


def build_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(description="ChunkMaker v1")
    subparsers = parser.add_subparsers(dest="command")

    run_parser = subparsers.add_parser("run", help="Run chunking")
    run_parser.add_argument("--input", "-i", help="Input CSV file or directory")
    run_parser.add_argument("--out", "-o", default="./out", help="Output directory")
    run_parser.add_argument("--max-chars", type=int, default=2500, help="Max chars per chunk")
    run_parser.add_argument("--overlap-chars", type=int, default=150, help="Overlap chars")
    run_parser.add_argument("--strategy", default="paragraphs", help="Chunking strategy")
    return parser


def open_output_files(out_dir: Path):
    out_dir.mkdir(parents=True, exist_ok=True)
    chunks_path = out_dir / "chunks.jsonl"
    tsv_path = out_dir / "chunks.tsv"
    meetings_path = out_dir / "meetings.jsonl"

    chunks_file = chunks_path.open("w", encoding="utf-8")
    tsv_file = tsv_path.open("w", encoding="utf-8")
    meetings_file = meetings_path.open("w", encoding="utf-8")

    tsv_file.write(
        "meeting_key\tdate_ymd\tmeeting_name\tchunk_index\tchunk_id\tchar_start\tchar_end\ttext\n"
    )
    return chunks_file, tsv_file, meetings_file


def generate_readme(out_dir: Path):
    content = (
        "# ChunkMaker Output\n\n"
        "Artifacts generated by `python -m chunkmaker run`.\n\n"
        "## Files\n"
        "- chunks.jsonl: chunk records, one JSON per line\n"
        "- chunks.tsv: tab-separated export for quick inspection\n"
        "- meetings.jsonl: meeting-level metadata\n\n"
        "## Run\n"
        "`python -m chunkmaker run --out out`\n"
    )
    (out_dir / "README.md").write_text(content, encoding="utf-8")


def run(args: argparse.Namespace) -> int:
    if args.strategy != "paragraphs":
        print("Only 'paragraphs' strategy is supported in v1.")
        return 1
    paths = discover_csv_files(args.input)
    if not paths:
        print("No CSV files found.")
        return 1

    records = iter_meeting_records(paths)
    has_records = False
    out_dir = Path(args.out)
    chunks_file, tsv_file, meetings_file = open_output_files(out_dir)
    total_chunks = 0
    meeting_count = 0
    longest = []

    for record in records:
        has_records = True
        normalized = normalize_text(record.transcript)
        record_chunks = chunk_text_from_normalized(
            record, normalized, args.max_chars, args.overlap_chars
        )
        for chunk in record_chunks:
            payload = {
                "meeting_key": chunk.meeting_key,
                "date_ymd": chunk.date_ymd,
                "meeting_name": chunk.meeting_name,
                "chunk_index": chunk.chunk_index,
                "chunk_id": chunk.chunk_id,
                "char_start": chunk.char_start,
                "char_end": chunk.char_end,
                "text": chunk.text,
            }
            chunks_file.write(json.dumps(payload, ensure_ascii=False) + "\n")
            tsv_text = chunk.text.replace("\t", " ").replace("\n", "\\n")
            tsv_file.write(
                f"{chunk.meeting_key}\t{chunk.date_ymd}\t{chunk.meeting_name}\t{chunk.chunk_index}"
                f"\t{chunk.chunk_id}\t{chunk.char_start}\t{chunk.char_end}\t{tsv_text}\n"
            )

        meeting_meta = MeetingMeta(
            meeting_key=record.meeting_key,
            date_ymd=record.date_ymd,
            meeting_name=record.meeting_name,
            transcript_char_len=len(normalized),
            chunk_count=len(record_chunks),
            source_file=record.source_file,
        )
        meetings_file.write(
            json.dumps(
                {
                    "meeting_key": meeting_meta.meeting_key,
                    "date_ymd": meeting_meta.date_ymd,
                    "meeting_name": meeting_meta.meeting_name,
                    "transcript_char_len": meeting_meta.transcript_char_len,
                    "chunk_count": meeting_meta.chunk_count,
                    "source_file": meeting_meta.source_file,
                },
                ensure_ascii=False,
            )
            + "\n"
        )

        total_chunks += len(record_chunks)
        meeting_count += 1
        longest.append((len(normalized), record.meeting_name))

    chunks_file.close()
    tsv_file.close()
    meetings_file.close()

    if not has_records:
        print("No meeting transcripts found in CSVs.")
        return 1

    generate_readme(out_dir)

    longest.sort(reverse=True)
    print(f"Meetings: {meeting_count}")
    print(f"Chunks: {total_chunks}")
    print("Top 5 longest transcripts:")
    for length, name in longest[:5]:
        print(f"- {length} chars: {name}")
    return 0


def main() -> int:
    parser = build_parser()
    args = parser.parse_args()
    if args.command != "run":
        parser.print_help()
        return 1
    return run(args)


if __name__ == "__main__":
    raise SystemExit(main())
